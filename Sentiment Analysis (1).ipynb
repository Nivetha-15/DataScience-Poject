{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style = 'color:green'> Capstone Project </span>\n",
    "#### <span style = 'color:blue'> Problem statement : Perform sentiment analysis on Omnicron variant, data fetching directly from twitter</span>\n",
    "**Sentiment analysis is the process of identifying feelings and emotions expressed in words, through ML or AI**\n",
    "\n",
    "**Project Pipeline**\n",
    "\n",
    "Various steps in completing project are\n",
    "\n",
    "- **Import Necessary Dependencies**\n",
    "- **Read and Load the Dataset**\n",
    "- **Exploratory Data Analysis**\n",
    "- **Data Visualization of Target Variables**\n",
    "- **Data Preprocessing**\n",
    "- **Splitting our data into Train and Test Subset**\n",
    "- **Transforming Dataset using TF-IDF Vectorizer**\n",
    "- **Function for Model Evaluation**\n",
    "- **Model Building**\n",
    "- **Conclusion**\n",
    "\n",
    "- Here we have to get dataset directly fetched from twitter in realtime \n",
    "\n",
    "- performing realtime sentimental analysis on realtime data collecting from twitter\n",
    "- objective: perform sentiment analysis on realtime data collected from twitter \n",
    "\n",
    "                      \n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style = 'color:blue'>   API (Application Programm Interface)</span>\n",
    "- Imagine you’re sitting at a table in a restaurant with a menu of choices to order from. The kitchen is the part of the “system” that will prepare your order. What is missing is the critical link to communicate your order to the kitchen and deliver your food back to your table. That’s where the waiter or API comes in. The waiter is the messenger – or API – that takes your request or order and tells the kitchen – the system – what to do. Then the waiter delivers the response back to you; in this case, it is the food.\n",
    "- API's are huge and are used everywhere\n",
    "- In simple words api stands as bridge for one to access the content in one's storage \n",
    "- There are many APIs on the Twitter platform that software developers can engage with, with the ultimate possibility to create fully automated systems which will interact with Twitter. While this feature could benefit companies by drawing insights from Twitter data\n",
    "\n",
    "   **From twitter api it's possible to extract many insights some are**\n",
    "- Tweets: searching, posting, filtering, engagement, streaming etc.\n",
    "- Accounts and users (Beta): account management, user interactions.\n",
    "- Media: uploading and accessing photos, videos and animated GIFs.\n",
    "- Trends: trending topics in a given location.\n",
    "- Geo: information about known places or places near a location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting twitter API keys**\n",
    "- If you don't already have an account, you can login with your normal Twitter credentials \n",
    "\n",
    "\n",
    "- follow the required prompts to create a developer project or click here <a href=\"https://dev.twitter.com/apps\" title=\"Twitter\">Click here</a>\n",
    "- Requesting the API key and secret via the Developer Portal causes Twitter to produce the following three things:\n",
    "1. API key (this is your 'consumer key')\n",
    "2. API secret key (this is your 'consumer secret')\n",
    "3. Bearer token\n",
    "- Next, visit the 'Authentication Tokens' area of the Developer Portal and generate an 'Access token & secret'. This will provide you with the following two items:\n",
    "1. Access token (this is your 'token key')\n",
    "2. Access token secret (this is your 'token secret')\n",
    "\n",
    "\n",
    "**Expected output**\n",
    "- the data fetched from twitter should undergo EDA for analyzing, cleaning, handling, manupulation, visualization..,etc\n",
    "- final output should show the sentiment of the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  *Some tips to consider*\n",
    "\n",
    "- Machines can learn in every possible way so its always better to think out of the box\n",
    "    \n",
    "- Perform eda as diverse as possible and in contineous manner\n",
    "    \n",
    "- Try configuring with diffrent models to know how each model is diffrent with other ones \n",
    "    \n",
    "- Donot try to involve unneccesory codes and useless algorithms for dataset which just increases complexity\n",
    "    \n",
    "- Approaching problem statement in n number of ways helps us to find best one possible\n",
    "    \n",
    "- It's easier for one to understnd and manupulate if we have models as simple as possible \n",
    "    \n",
    "- When we have multiple models we can have multiple judgements based on models and their efficiencies\n",
    "    \n",
    "- Tuning helps increasing accuracy :)\n",
    "    \n",
    "- Have an idea of time consumed by the model, its better to have a model whose time management is good\n",
    "    \n",
    "- Spend good amount of time on analyzing dataset and draw as much insights as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tweepy is importantlibrary we will using to fetch data from twitter by api\n",
    "\n",
    "\n",
    "For more on tweepy documentation please click here <a href=\"https://docs.tweepy.org/en/stable/getting_started.html#hello-tweepy\" title=\"Tweepy\">Click here</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweepy\n",
      "  Obtaining dependency information for tweepy from https://files.pythonhosted.org/packages/4d/78/ba0065d5636bbf4a35b78c4f81b74e7858b609cdf69e629d6da5c91b9d92/tweepy-4.14.0-py3-none-any.whl.metadata\n",
      "  Downloading tweepy-4.14.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting oauthlib<4,>=3.2.0 (from tweepy)\n",
      "  Obtaining dependency information for oauthlib<4,>=3.2.0 from https://files.pythonhosted.org/packages/7e/80/cab10959dc1faead58dc8384a781dfbf93cb4d33d50988f7a69f1b7c9bbe/oauthlib-3.2.2-py3-none-any.whl.metadata\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in c:\\users\\nivet\\anaconda3\\lib\\site-packages (from tweepy) (2.31.0)\n",
      "Collecting requests-oauthlib<2,>=1.2.0 (from tweepy)\n",
      "  Obtaining dependency information for requests-oauthlib<2,>=1.2.0 from https://files.pythonhosted.org/packages/6f/bb/5deac77a9af870143c684ab46a7934038a53eb4aa975bc0687ed6ca2c610/requests_oauthlib-1.3.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nivet\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nivet\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nivet\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nivet\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2023.7.22)\n",
      "Downloading tweepy-4.14.0-py3-none-any.whl (98 kB)\n",
      "   ---------------------------------------- 0.0/98.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 98.5/98.5 kB 2.8 MB/s eta 0:00:00\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "   ---------------------------------------- 0.0/151.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 151.7/151.7 kB 8.8 MB/s eta 0:00:00\n",
      "Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Installing collected packages: oauthlib, requests-oauthlib, tweepy\n",
      "Successfully installed oauthlib-3.2.2 requests-oauthlib-1.3.1 tweepy-4.14.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tweepy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nivet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace these with your own Twitter API credentials\n",
    "API_KEY = 'your_api_key'\n",
    "API_SECRET_KEY = 'your_api_secret_key'\n",
    "ACCESS_TOKEN = 'your_access_token'\n",
    "ACCESS_TOKEN_SECRET = 'your_access_token_secret'\n",
    "\n",
    "# Authentication with Twitter API\n",
    "auth = tweepy.OAuth1UserHandler(API_KEY, API_SECRET_KEY, ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "api = tweepy.API(auth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Unauthorized",
     "evalue": "401 Unauthorized\nUnauthorized",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnauthorized\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m num_tweets \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Fetch tweets\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m tweets \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39msearch_recent_tweets(query\u001b[38;5;241m=\u001b[39msearch_term, max_results\u001b[38;5;241m=\u001b[39mnum_tweets, tweet_fields\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreated_at\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Create a DataFrame\u001b[39;00m\n\u001b[0;32m     16\u001b[0m tweets_data \u001b[38;5;241m=\u001b[39m [[tweet\u001b[38;5;241m.\u001b[39mtext, tweet\u001b[38;5;241m.\u001b[39mcreated_at] \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m tweets\u001b[38;5;241m.\u001b[39mdata]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tweepy\\client.py:1266\u001b[0m, in \u001b[0;36mClient.search_recent_tweets\u001b[1;34m(self, query, user_auth, **params)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"search_recent_tweets( \\\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;124;03m    query, *, end_time=None, expansions=None, max_results=None, \\\u001b[39;00m\n\u001b[0;32m   1176\u001b[0m \u001b[38;5;124;03m    media_fields=None, next_token=None, place_fields=None, \\\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1263\u001b[0m \u001b[38;5;124;03m.. _Academic Research Project: https://developer.twitter.com/en/docs/projects\u001b[39;00m\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1265\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m query\n\u001b[1;32m-> 1266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m   1267\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/2/tweets/search/recent\u001b[39m\u001b[38;5;124m\"\u001b[39m, params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m   1268\u001b[0m     endpoint_parameters\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1269\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend_time\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpansions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_results\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedia.fields\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1270\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_token\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplace.fields\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpoll.fields\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1271\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msince_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msort_order\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_time\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtweet.fields\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1272\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muntil_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser.fields\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1273\u001b[0m     ), data_type\u001b[38;5;241m=\u001b[39mTweet, user_auth\u001b[38;5;241m=\u001b[39muser_auth\n\u001b[0;32m   1274\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tweepy\\client.py:129\u001b[0m, in \u001b[0;36mBaseClient._make_request\u001b[1;34m(self, method, route, params, endpoint_parameters, json, data_type, user_auth)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_request\u001b[39m(\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m, method, route, params\u001b[38;5;241m=\u001b[39m{}, endpoint_parameters\u001b[38;5;241m=\u001b[39m(), json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    125\u001b[0m     data_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, user_auth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    126\u001b[0m ):\n\u001b[0;32m    127\u001b[0m     request_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_params(params, endpoint_parameters)\n\u001b[1;32m--> 129\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(method, route, params\u001b[38;5;241m=\u001b[39mrequest_params,\n\u001b[0;32m    130\u001b[0m                             json\u001b[38;5;241m=\u001b[39mjson, user_auth\u001b[38;5;241m=\u001b[39muser_auth)\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_type \u001b[38;5;129;01mis\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tweepy\\client.py:98\u001b[0m, in \u001b[0;36mBaseClient.request\u001b[1;34m(self, method, route, params, json, user_auth)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadRequest(response)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m:\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Unauthorized(response)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Forbidden(response)\n",
      "\u001b[1;31mUnauthorized\u001b[0m: 401 Unauthorized\nUnauthorized"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "\n",
    "# Bearer Token Authentication\n",
    "BEARER_TOKEN = 'your_bearer_token'\n",
    "\n",
    "client = tweepy.Client(bearer_token=BEARER_TOKEN)\n",
    "\n",
    "# Define search term and number of tweets\n",
    "search_term = \"Omicron\"\n",
    "num_tweets = 1000\n",
    "\n",
    "# Fetch tweets\n",
    "tweets = client.search_recent_tweets(query=search_term, max_results=num_tweets, tweet_fields=['created_at', 'text'])\n",
    "\n",
    "# Create a DataFrame\n",
    "tweets_data = [[tweet.text, tweet.created_at] for tweet in tweets.data]\n",
    "df = pd.DataFrame(tweets_data, columns=['text', 'created_at'])\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during authentication: 401 Unauthorized\n",
      "89 - Invalid or expired token.\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "\n",
    "# Your regenerated credentials\n",
    "API_KEY = 'your_new_api_key'\n",
    "API_SECRET_KEY = 'your_new_api_secret_key'\n",
    "ACCESS_TOKEN = 'your_new_access_token'\n",
    "ACCESS_TOKEN_SECRET = 'your_new_access_token_secret'\n",
    "\n",
    "# Authenticate to Twitter\n",
    "auth = tweepy.OAuth1UserHandler(API_KEY, API_SECRET_KEY, ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# Test API request - Get your own account information\n",
    "try:\n",
    "    user = api.verify_credentials()\n",
    "    if user:\n",
    "        print(\"Authentication successful!\")\n",
    "        print(f\"Logged in as: {user.screen_name}\")\n",
    "    else:\n",
    "        print(\"Authentication failed.\")\n",
    "except tweepy.TweepyException as e:\n",
    "    print(f\"Error during authentication: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)])\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[1;32m---> 11\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(clean_text)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'#\\w+', '', text)  # Remove hashtags\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords.words('english')])\n",
    "    return text\n",
    "\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Sentiment distribution (initial, if you have labels)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# sns.countplot(df['sentiment'])\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Word Cloud Visualization\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[0;32m      8\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m      9\u001b[0m wordcloud \u001b[38;5;241m=\u001b[39m WordCloud(width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m800\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;241m.\u001b[39mgenerate(text)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Sentiment distribution (initial, if you have labels)\n",
    "# sns.countplot(df['sentiment'])\n",
    "# plt.show()\n",
    "\n",
    "# Word Cloud Visualization\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "text = ' '.join(df['cleaned_text'].tolist())\n",
    "wordcloud = WordCloud(width=800, height=400, max_words=100).generate(text)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Since this is unsupervised data, consider manually labeling or using a pre-trained sentiment analyzer for labels\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# For demonstration purposes, assume we have a 'sentiment' column with labels\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Example of creating dummy labels (not recommended for real analysis)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositive\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneutral\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m'\u001b[39m], size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(df))\n\u001b[0;32m      7\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m], df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m], test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Since this is unsupervised data, consider manually labeling or using a pre-trained sentiment analyzer for labels\n",
    "# For demonstration purposes, assume we have a 'sentiment' column with labels\n",
    "\n",
    "# Example of creating dummy labels (not recommended for real analysis)\n",
    "df['sentiment'] = np.random.choice(['positive', 'neutral', 'negative'], size=len(df))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_text'], df['sentiment'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m tfidf \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m X_train_tfidf \u001b[38;5;241m=\u001b[39m tfidf\u001b[38;5;241m.\u001b[39mfit_transform(X_train)\n\u001b[0;32m      3\u001b[0m X_test_tfidf \u001b[38;5;241m=\u001b[39m tfidf\u001b[38;5;241m.\u001b[39mtransform(X_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[1;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train_tfidf, y_train)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Predicting on the test set\u001b[39;00m\n\u001b[0;32m      5\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_tfidf)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
